"""å¯¼å‡ºæ•°æ®åº“æ•°æ®åˆ°æ–‡ä»¶çš„è„šæœ¬."""

import asyncio
import os
from pathlib import Path

import dotenv

from cryptoservice.models.enums import Freq
from cryptoservice.models.universe import UniverseDefinition
from cryptoservice.services.market_service import MarketDataService
from cryptoservice.storage import AsyncMarketDB

# ============== é…ç½®å‚æ•° ==============
# æ–‡ä»¶è·¯å¾„
UNIVERSE_FILE = "./data/universe.json"  # Universeå®šä¹‰æ–‡ä»¶
DB_PATH = "./data/database/market.db"  # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
EXPORT_BASE_PATH = "./data/exports"  # å¯¼å‡ºæ–‡ä»¶åŸºç¡€è·¯å¾„

# å¯¼å‡ºé…ç½®
DATA_FREQ = Freq.d1  # æ•°æ®åº“æ•°æ®é¢‘ç‡
EXPORT_FREQ = Freq.d1  # å¯¼å‡ºæ•°æ®é¢‘ç‡
CHUNK_DAYS = 100  # åˆ†å—å¤©æ•°
DOWNLOAD_CATEGORIES = True  # æ˜¯å¦ä¸‹è½½ç±»åˆ«æ•°æ®ï¼ˆç½‘ç»œè¿æ¥å¤±è´¥æ—¶ä¼šè‡ªåŠ¨è·³è¿‡ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ä¸ºFalseï¼‰

# å¯¼å‡ºçš„ç‰¹å¾ï¼ˆçŸ­å­—æ®µåæ ¼å¼ï¼ŒæŒ‰æŒ‡å®šé¡ºåºï¼‰
EXPORT_FEATURES = [
    "cls",
    "hgh",
    "low",
    "tnum",
    "opn",
    "amt",
    "tbvol",
    "tbamt",
    "vol",
    "vwap",
    "ret",
    "tsvol",
    "tsamt",
    # æ–°ç‰¹å¾ï¼ˆä¸‰ä¸ªæ ¸å¿ƒç‰¹å¾ï¼‰
    "fr",  # èµ„é‡‘è´¹ç‡
    "oi",  # æŒä»“é‡
    "lsr",  # å¤šç©ºæ¯”ä¾‹
]

# ç‰¹å¾æè¿°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
FEATURE_DESCRIPTIONS = {
    "cls": "æ”¶ç›˜ä»·",
    "hgh": "æœ€é«˜ä»·",
    "low": "æœ€ä½ä»·",
    "tnum": "äº¤æ˜“ç¬”æ•°",
    "opn": "å¼€ç›˜ä»·",
    "amt": "æˆäº¤é¢",
    "tbvol": "ä¸»åŠ¨ä¹°å…¥é‡",
    "tbamt": "ä¸»åŠ¨ä¹°å…¥é¢",
    "vol": "æˆäº¤é‡",
    "vwap": "VWAP",
    "ret": "æ”¶ç›Šç‡",
    "tsvol": "ä¸»åŠ¨å–å‡ºé‡",
    "tsamt": "ä¸»åŠ¨å–å‡ºé¢",
    # æ–°ç‰¹å¾æè¿°
    "fr": "èµ„é‡‘è´¹ç‡",
    "oi": "æŒä»“é‡",
    "lsr": "å¤šç©ºæ¯”ä¾‹",
}

# ========================================


async def validate_prerequisites() -> tuple[UniverseDefinition, bool]:
    """éªŒè¯å¿…è¦æ–‡ä»¶å¹¶åŠ è½½Universeå®šä¹‰."""
    if not Path(UNIVERSE_FILE).exists():
        print(f"âŒ Universeæ–‡ä»¶ä¸å­˜åœ¨: {UNIVERSE_FILE}")
        print("è¯·å…ˆè¿è¡Œ define_universe.py åˆ›å»ºUniverseæ–‡ä»¶")
        raise FileNotFoundError(f"Universe file not found: {UNIVERSE_FILE}")

    if not Path(DB_PATH).exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {DB_PATH}")
        print("è¯·å…ˆè¿è¡Œ download_data.py ä¸‹è½½æ•°æ®")
        raise FileNotFoundError(f"Database file not found: {DB_PATH}")

    # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
    Path(EXPORT_BASE_PATH).mkdir(parents=True, exist_ok=True)

    # åŠ è½½Universeå®šä¹‰
    print("ğŸ“– åŠ è½½Universeå®šä¹‰...")
    universe_def = UniverseDefinition.load_from_file(UNIVERSE_FILE)
    print(f"   âœ… æˆåŠŸåŠ è½½ {len(universe_def.snapshots)} ä¸ªå¿«ç…§")

    return universe_def, True


async def initialize_market_service():
    """åˆå§‹åŒ–å¸‚åœºæœåŠ¡ï¼ˆå¦‚æœéœ€è¦ä¸‹è½½åˆ†ç±»æ•°æ®ï¼‰."""
    if not DOWNLOAD_CATEGORIES:
        return None

    try:
        print("ğŸ”— åˆå§‹åŒ–å¸‚åœºæœåŠ¡ï¼ˆç”¨äºä¸‹è½½åˆ†ç±»æ•°æ®ï¼‰...")
        dotenv.load_dotenv()
        api_key = os.getenv("BINANCE_API_KEY")
        api_secret = os.getenv("BINANCE_API_SECRET")
        market_service_ctx = await MarketDataService.create(api_key=api_key, api_secret=api_secret)
        print("   âœ… å¸‚åœºæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        return market_service_ctx
    except Exception as e:
        print(f"   âš ï¸ å¸‚åœºæœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡åˆ†ç±»æ•°æ®ä¸‹è½½: {e}")
        print("   ğŸ’¡ æç¤ºï¼šå¦‚éœ€ä¸‹è½½åˆ†ç±»æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
        return None


async def process_snapshot(snapshot, snapshot_id, total_snapshots, universe_config, db, market_service_ctx):
    """å¤„ç†å•ä¸ªå¿«ç…§çš„å¯¼å‡º."""
    print(f"\nğŸ“‹ å¤„ç†å¿«ç…§ {snapshot_id}/{total_snapshots}: {snapshot.start_date} - {snapshot.end_date}")

    start_date_ts = snapshot.start_date_ts
    end_date_ts = snapshot.end_date_ts
    symbols = snapshot.symbols

    print(f"   â° æ—¶é—´èŒƒå›´: {start_date_ts} - {end_date_ts}")
    print(f"   ğŸ’± äº¤æ˜“å¯¹æ•°é‡: {len(symbols)}")
    print(f"   ğŸ“ å‰5ä¸ªäº¤æ˜“å¯¹: {symbols[:5]}")

    # åˆ›å»ºå¿«ç…§ä¸“ç”¨çš„å¯¼å‡ºç›®å½•
    config = universe_config
    top_value = f"k{config.top_k}" if config.top_k else f"r{config.top_ratio}"
    dir_name = f"univ_{config.t1_months}_{config.t2_months}_{config.t3_months}_{top_value}"
    snapshot_export_path = Path(EXPORT_BASE_PATH) / dir_name

    # åˆå§‹åŒ–ç»“æœçŠ¶æ€
    result = {
        "snapshot_id": snapshot_id,
        "date_range": f"{snapshot.start_date} - {snapshot.end_date}",
        "data_export": False,
        "category_export": False,
        "export_path": snapshot_export_path,
        "error": None,
    }

    try:
        # å¯¼å‡ºæ•°æ®
        await db.export_to_files_by_timestamp(
            output_path=snapshot_export_path,
            start_ts=start_date_ts,
            end_ts=end_date_ts,
            freq=DATA_FREQ,
            target_freq=EXPORT_FREQ,
            symbols=symbols,
            chunk_days=CHUNK_DAYS,
        )
        result["data_export"] = True
        print("   âœ… ä¸»è¦æ•°æ®å¯¼å‡ºæˆåŠŸ")

        # å°è¯•ä¸‹è½½åˆ†ç±»æ•°æ®
        if DOWNLOAD_CATEGORIES and market_service_ctx is not None:
            try:
                print("   ğŸ“Š ä¸‹è½½åˆ†ç±»æ•°æ®...")
                async with market_service_ctx as market_service:
                    market_service.download_and_save_categories_for_universe(
                        universe_file=UNIVERSE_FILE,
                        output_path=snapshot_export_path,
                    )
                result["category_export"] = True
                print("   âœ… åˆ†ç±»æ•°æ®ä¸‹è½½æˆåŠŸ")
            except Exception as e:
                print(f"   âš ï¸ åˆ†ç±»æ•°æ®ä¸‹è½½å¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")
                print("   ğŸ’¡ æç¤ºï¼šå¯ä»¥ç¨åå•ç‹¬ä¸‹è½½åˆ†ç±»æ•°æ®ï¼Œæˆ–è®¾ç½® DOWNLOAD_CATEGORIES = False")
        elif DOWNLOAD_CATEGORIES and market_service_ctx is None:
            print("   âš ï¸ è·³è¿‡åˆ†ç±»æ•°æ®ä¸‹è½½ï¼ˆå¸‚åœºæœåŠ¡æœªåˆå§‹åŒ–ï¼‰")

        # æ˜¾ç¤ºå¯¼å‡ºçš„æ–‡ä»¶ä¿¡æ¯
        display_export_info(snapshot_export_path)

    except Exception as e:
        result["error"] = str(e)
        print(f"   âŒ å¿«ç…§å¯¼å‡ºå¤±è´¥: {e}")

    return result


def display_export_info(snapshot_export_path):
    """æ˜¾ç¤ºå¯¼å‡ºæ–‡ä»¶ä¿¡æ¯."""
    if snapshot_export_path.exists():
        export_files = list(snapshot_export_path.rglob("*.npy"))
        universe_files = list(snapshot_export_path.rglob("universe_token.pkl"))

        if export_files:
            total_size = sum(f.stat().st_size for f in export_files) / (1024 * 1024)  # MB
            print(f"      ğŸ“Š å¯¼å‡ºæ–‡ä»¶æ•°é‡: {len(export_files)}ä¸ª.npyæ–‡ä»¶")
            print(f"      ğŸ¯ Universeæ–‡ä»¶: {len(universe_files)}ä¸ª.pklæ–‡ä»¶")
            print(f"      ğŸ’¾ æ€»æ–‡ä»¶å¤§å°: {total_size:.1f} MB")

            # æ˜¾ç¤ºç‰¹å¾åˆ†å¸ƒ
            feature_dirs = [f.parent.name for f in export_files]
            unique_features = set(feature_dirs)
            print(f"      ğŸ“ˆ ç‰¹å¾ç±»å‹: {len(unique_features)}ç§ ({', '.join(sorted(unique_features))})")


def print_final_summary(export_results):
    """æ‰“å°æœ€ç»ˆæ±‡æ€»."""
    print("\n" + "=" * 60)
    print("ğŸ¯ å¯¼å‡ºå®Œæˆæ±‡æ€»:")
    successful_data = sum(1 for r in export_results if r["data_export"])
    successful_categories = sum(1 for r in export_results if r["category_export"])
    total_snapshots = len(export_results)

    print(f"   ğŸ“Š æ€»å¿«ç…§æ•°: {total_snapshots}")
    print(f"   âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ: {successful_data}/{total_snapshots}")
    print(f"   ğŸ“‚ åˆ†ç±»æ•°æ®æˆåŠŸ: {successful_categories}/{total_snapshots}")

    if successful_data == total_snapshots:
        print("   ğŸ‰ æ‰€æœ‰æ•°æ®å¯¼å‡ºæˆåŠŸï¼")
    else:
        print("   âš ï¸ éƒ¨åˆ†å¿«ç…§å¯¼å‡ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

    if DOWNLOAD_CATEGORIES and successful_categories < total_snapshots:
        print("   ğŸ’¡ æç¤ºï¼šå¦‚éœ€é‡æ–°ä¸‹è½½åˆ†ç±»æ•°æ®ï¼Œå¯å•ç‹¬è¿è¡Œæˆ–è°ƒæ•´ç½‘ç»œè®¾ç½®")

    print("=" * 60)


async def main():
    """ä»æ•°æ®åº“å¯¼å‡ºæ•°æ®è„šæœ¬."""
    print("ğŸ“¤ å¼€å§‹ä»æ•°æ®åº“å¯¼å‡ºæ•°æ®")
    print(f"ğŸ“‹ Universeæ–‡ä»¶: {UNIVERSE_FILE}")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    print(f"ğŸ“ å¯¼å‡ºè·¯å¾„: {EXPORT_BASE_PATH}")
    print(f"â±ï¸ å¯¼å‡ºé¢‘ç‡: {EXPORT_FREQ}")
    print(f"ğŸ“Š å¯¼å‡ºç‰¹å¾: {len(EXPORT_FEATURES)}ä¸ª")
    print(f"    {', '.join([f'{feat}({FEATURE_DESCRIPTIONS[feat]})' for feat in EXPORT_FEATURES[:5]])}...")
    print(f"ğŸŒ åˆ†ç±»æ•°æ®ä¸‹è½½: {'å¯ç”¨' if DOWNLOAD_CATEGORIES else 'ç¦ç”¨'}ï¼ˆç½‘ç»œè¿æ¥å¤±è´¥æ—¶ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰")

    try:
        # éªŒè¯å‰ææ¡ä»¶å¹¶åŠ è½½Universe
        universe_def, _ = await validate_prerequisites()

        # åˆå§‹åŒ–å¸‚åœºæœåŠ¡
        market_service_ctx = await initialize_market_service()

        # åˆ›å»ºMarketDBå®ä¾‹
        db = AsyncMarketDB(DB_PATH)
        export_results = []

        try:
            # å¤„ç†æ¯ä¸ªå¿«ç…§
            for i, snapshot in enumerate(universe_def.snapshots):
                result = await process_snapshot(
                    snapshot, i + 1, len(universe_def.snapshots), universe_def.config, db, market_service_ctx
                )
                export_results.append(result)

        finally:
            await db.close()

        # æ‰“å°æœ€ç»ˆæ±‡æ€»
        print_final_summary(export_results)

    except Exception as e:
        print(f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    asyncio.run(main())

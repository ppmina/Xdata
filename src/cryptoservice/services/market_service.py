"""å¸‚åœºæ•°æ®æœåŠ¡æ¨¡å—ã€‚

æä¾›åŠ å¯†è´§å¸å¸‚åœºæ•°æ®è·å–ã€å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚
"""

import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional
from decimal import Decimal
import pandas as pd

from rich.progress import Progress

from cryptoservice.client import BinanceClientFactory
from cryptoservice.config import settings, RetryConfig
from cryptoservice.data import MarketDB
from cryptoservice.exceptions import (
    InvalidSymbolError,
    MarketDataFetchError,
)
from cryptoservice.interfaces import IMarketDataService
from cryptoservice.models import (
    DailyMarketTicker,
    Freq,
    HistoricalKlinesType,
    KlineMarketTicker,
    PerpetualMarketTicker,
    SortBy,
    SymbolTicker,
    UniverseConfig,
    UniverseDefinition,
    UniverseSnapshot,
    ErrorSeverity,
    IntegrityReport,
)
from cryptoservice.utils import (
    DataConverter,
    RateLimitManager,
    ExponentialBackoff,
    EnhancedErrorHandler,
    TimeUtils,
    logger,
    OutputMode,
    time_cache,
    symbol_cache,
    calculation_cache,
)


class MarketDataService(IMarketDataService):
    """å¸‚åœºæ•°æ®æœåŠ¡å®ç°ç±»ã€‚"""

    def __init__(self, api_key: str, api_secret: str) -> None:
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡ã€‚

        Args:
            api_key: ç”¨æˆ·APIå¯†é’¥
            api_secret: ç”¨æˆ·APIå¯†é’¥
        """
        self.client = BinanceClientFactory.create_client(api_key, api_secret)
        self.converter = DataConverter()
        self.db: MarketDB | None = None

        # ä¸åŒAPIç«¯ç‚¹çš„é¢‘ç‡é™åˆ¶ç®¡ç†å™¨
        self.rate_limit_manager = RateLimitManager(base_delay=0.5, max_requests_per_minute=1500)  # é»˜è®¤ç®¡ç†å™¨
        self.spot_rate_manager = RateLimitManager(base_delay=0.6, max_requests_per_minute=1200)  # ç°è´§API (1200/min)
        self.futures_rate_manager = RateLimitManager(base_delay=0.3, max_requests_per_minute=1800)  # åˆçº¦API (2400/min)
        self.heavy_rate_manager = RateLimitManager(base_delay=1.0, max_requests_per_minute=600)  # é«˜æƒé‡API
        self.batch_rate_manager = RateLimitManager(base_delay=0.8, max_requests_per_minute=1000)  # æ‰¹é‡æ“ä½œ

        # ç¼“å­˜ç®¡ç†
        self._cache_managers = {
            "time_cache": time_cache,
            "symbol_cache": symbol_cache,
            "calculation_cache": calculation_cache,
        }

    def get_cache_stats(self) -> dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ã€‚

        Returns:
            dict: å„ä¸ªç¼“å­˜çš„ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {}
        for cache_name, cache_decorator in self._cache_managers.items():
            try:
                stats[cache_name] = cache_decorator.cache_stats()  # type: ignore
            except Exception as e:
                stats[cache_name] = {"error": str(e)}
        return stats

    def clear_all_caches(self) -> None:
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜ã€‚"""
        for cache_name, cache_decorator in self._cache_managers.items():
            try:
                cache_decorator.cache_clear()  # type: ignore
                logger.info(f"å·²æ¸…ç† {cache_name} ç¼“å­˜")
            except Exception as e:
                logger.warning(f"æ¸…ç† {cache_name} ç¼“å­˜æ—¶å‡ºé”™: {e}")

    def cleanup_expired_caches(self) -> dict[str, int]:
        """æ¸…ç†è¿‡æœŸç¼“å­˜ã€‚

        Returns:
            dict: å„ä¸ªç¼“å­˜æ¸…ç†çš„è¿‡æœŸé¡¹æ•°é‡
        """
        cleanup_stats = {}
        for cache_name, cache_decorator in self._cache_managers.items():
            try:
                expired_count = cache_decorator.cache_cleanup()  # type: ignore
                cleanup_stats[cache_name] = expired_count
                if expired_count > 0:
                    logger.info(f"{cache_name} æ¸…ç†äº† {expired_count} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")
            except Exception as e:
                cleanup_stats[cache_name] = 0
                logger.warning(f"æ¸…ç† {cache_name} è¿‡æœŸç¼“å­˜æ—¶å‡ºé”™: {e}")
        return cleanup_stats

    def _validate_and_prepare_path(self, path: Path | str, is_file: bool = False, file_name: str | None = None) -> Path:
        """éªŒè¯å¹¶å‡†å¤‡è·¯å¾„ã€‚

        Args:
            path: è·¯å¾„å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡
            is_file: æ˜¯å¦ä¸ºæ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
        Returns:
            Path: éªŒè¯åçš„Pathå¯¹è±¡

        Raises:
            ValueError: è·¯å¾„ä¸ºç©ºæˆ–æ— æ•ˆæ—¶
        """
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®š")

        path_obj = Path(path)

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        if is_file:
            if path_obj.is_dir():
                path_obj = path_obj.joinpath(file_name) if file_name else path_obj
            else:
                path_obj.parent.mkdir(parents=True, exist_ok=True)
        else:
            # å¦‚æœæ˜¯ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            path_obj.mkdir(parents=True, exist_ok=True)

        return path_obj

    def get_symbol_ticker(self, symbol: str | None = None) -> SymbolTicker | list[SymbolTicker]:
        """è·å–å•ä¸ªæˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°

        Returns:
            SymbolTicker | list[SymbolTicker]: å•ä¸ªäº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®æˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®
        """
        try:
            # ç°è´§APIè°ƒç”¨ï¼Œä½¿ç”¨ç°è´§é¢‘ç‡ç®¡ç†å™¨
            self.spot_rate_manager.wait_before_request()

            ticker = self.client.get_symbol_ticker(symbol=symbol)
            if not ticker:
                raise InvalidSymbolError(f"Invalid symbol: {symbol}")

            if isinstance(ticker, list):
                result: SymbolTicker | list[SymbolTicker] = [SymbolTicker.from_binance_ticker(t) for t in ticker]
            else:
                result = SymbolTicker.from_binance_ticker(ticker)

            self.spot_rate_manager.handle_success()
            return result

        except Exception as e:
            logger.error(f"[red]Error fetching ticker for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to fetch ticker: {e}") from e

    def get_perpetual_symbols(self, only_trading: bool = True, quote_asset: str = "USDT") -> list[str]:
        """è·å–å½“å‰å¸‚åœºä¸Šæ‰€æœ‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            only_trading: æ˜¯å¦åªè¿”å›å½“å‰å¯äº¤æ˜“çš„äº¤æ˜“å¯¹
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªè¿”å›ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            list[str]: æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"è·å–å½“å‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆç­›é€‰æ¡ä»¶ï¼š{quote_asset}ç»“å°¾ï¼‰")

            # åˆçº¦APIè°ƒç”¨ï¼Œä½¿ç”¨åˆçº¦é¢‘ç‡ç®¡ç†å™¨
            self.futures_rate_manager.wait_before_request()

            futures_info = self.client.futures_exchange_info()
            perpetual_symbols = [
                symbol["symbol"]
                for symbol in futures_info["symbols"]
                if symbol["contractType"] == "PERPETUAL"
                and (not only_trading or symbol["status"] == "TRADING")
                and symbol["symbol"].endswith(quote_asset)
            ]

            self.futures_rate_manager.handle_success()
            logger.success(f"æ‰¾åˆ° {len(perpetual_symbols)} ä¸ª{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹")
            return perpetual_symbols

        except Exception as e:
            logger.error(f"[red]è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}") from e

    @time_cache
    def _date_to_timestamp_range(self, date: str, interval: Freq | None = None) -> tuple[str, str]:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
            interval: æ—¶é—´é—´éš”ï¼Œç”¨äºç¡®å®šåˆé€‚çš„æˆªæ­¢æ—¶é—´

        Returns:
            tuple[str, str]: (å¼€å§‹æ—¶é—´æˆ³, ç»“æŸæ—¶é—´æˆ³)ï¼Œéƒ½æ˜¯æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
            - å¼€å§‹æ—¶é—´æˆ³: å½“å¤©çš„ 00:00:00
            - ç»“æŸæ—¶é—´æˆ³: å¯¹åº”æ—¶é—´é—´éš”çš„æ—¥æˆªæ­¢æ—¶é—´
        """
        return TimeUtils.date_to_timestamp_range(date, interval)

    @time_cache
    def _date_to_timestamp_start(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©å¼€å§‹çš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 00:00:00 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        return TimeUtils.date_to_timestamp_start(date)

    @time_cache
    def _date_to_timestamp_end(self, date: str, interval: Freq | None = None) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯¹åº”æ—¶é—´é—´éš”çš„æ—¥æˆªæ­¢æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
            interval: æ—¶é—´é—´éš”ï¼Œç”¨äºç¡®å®šåˆé€‚çš„æˆªæ­¢æ—¶é—´

        Returns:
            str: å¯¹åº”æ—¶é—´é—´éš”çš„æ—¥æˆªæ­¢æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        """
        return TimeUtils.date_to_timestamp_end(date, interval)

    @symbol_cache
    def check_symbol_exists_on_date(self, symbol: str, date: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            date: æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            bool: æ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹
        """
        try:
            # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´ï¼ˆä½¿ç”¨æ—¥çº¿é—´éš”ï¼‰
            start_time, end_time = self._date_to_timestamp_range(date, Freq.d1)

            # åˆçº¦APIè°ƒç”¨ï¼Œä½¿ç”¨åˆçº¦é¢‘ç‡ç®¡ç†å™¨
            self.futures_rate_manager.wait_before_request()

            # å°è¯•è·å–è¯¥æ—¶é—´èŒƒå›´å†…çš„Kçº¿æ•°æ®
            klines = self.client.futures_klines(
                symbol=symbol,
                interval="1d",
                startTime=start_time,
                endTime=end_time,
                limit=1,
            )

            # å¦‚æœæœ‰æ•°æ®ï¼Œè¯´æ˜è¯¥æ—¥æœŸå­˜åœ¨è¯¥äº¤æ˜“å¯¹
            result = bool(klines and len(klines) > 0)

            self.futures_rate_manager.handle_success()
            return result

        except Exception as e:
            logger.debug(f"æ£€æŸ¥äº¤æ˜“å¯¹ {symbol} åœ¨ {date} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    def get_top_coins(
        self,
        limit: int = settings.DEFAULT_LIMIT,
        sort_by: SortBy = SortBy.QUOTE_VOLUME,
        quote_asset: str | None = None,
    ) -> list[DailyMarketTicker]:
        """è·å–å‰Nä¸ªäº¤æ˜“å¯¹ã€‚

        Args:
            limit: æ•°é‡
            sort_by: æ’åºæ–¹å¼
            quote_asset: åŸºå‡†èµ„äº§

        Returns:
            list[DailyMarketTicker]: å‰Nä¸ªäº¤æ˜“å¯¹
        """
        try:
            # ç°è´§APIè°ƒç”¨ï¼Œä½¿ç”¨ç°è´§é¢‘ç‡ç®¡ç†å™¨
            self.spot_rate_manager.wait_before_request()

            tickers = self.client.get_ticker()
            market_tickers = [DailyMarketTicker.from_binance_ticker(t) for t in tickers]

            if quote_asset:
                market_tickers = [t for t in market_tickers if t.symbol.endswith(quote_asset)]

            result = sorted(
                market_tickers,
                key=lambda x: getattr(x, sort_by.value),
                reverse=True,
            )[:limit]

            self.spot_rate_manager.handle_success()
            return result

        except Exception as e:
            logger.error(f"[red]Error getting top coins: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get top coins: {e}") from e

    def get_market_summary(self, interval: Freq = Freq.d1) -> dict[str, Any]:
        """è·å–å¸‚åœºæ¦‚è§ˆã€‚

        Args:
            interval: æ—¶é—´é—´éš”

        Returns:
            dict[str, Any]: å¸‚åœºæ¦‚è§ˆ
        """
        try:
            summary: dict[str, Any] = {"snapshot_time": datetime.now(), "data": {}}
            # get_symbol_ticker å·²ç»å†…ç½®äº†é¢‘ç‡é™åˆ¶ç®¡ç†
            tickers_result = self.get_symbol_ticker()
            if isinstance(tickers_result, list):
                tickers = [ticker.to_dict() for ticker in tickers_result]
            else:
                tickers = [tickers_result.to_dict()]
            summary["data"] = tickers

            return summary

        except Exception as e:
            logger.error(f"[red]Error getting market summary: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get market summary: {e}") from e

    def get_historical_klines(
        self,
        symbol: str,
        start_time: str | datetime,
        end_time: str | datetime | None = None,
        interval: Freq = Freq.h1,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.SPOT,
    ) -> list[KlineMarketTicker]:
        """è·å–å†å²è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ºå½“å‰æ—¶é—´
            interval: æ—¶é—´é—´éš”
            klines_type: Kçº¿ç±»å‹ï¼ˆç°è´§æˆ–æœŸè´§ï¼‰

        Returns:
            list[KlineMarketTicker]: å†å²è¡Œæƒ…æ•°æ®
        """
        try:
            # å¤„ç†æ—¶é—´æ ¼å¼
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if end_time is None:
                end_time = datetime.now()
            elif isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)

            # è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time.strftime("%Y-%m-%d"))
            end_ts = self._date_to_timestamp_end(end_time.strftime("%Y-%m-%d"), interval)

            logger.info(f"è·å– {symbol} çš„å†å²æ•°æ® ({interval.value})")

            # æ ¹æ®klines_typeé€‰æ‹©åˆé€‚çš„é¢‘ç‡ç®¡ç†å™¨å’ŒAPI
            if klines_type == HistoricalKlinesType.FUTURES:
                rate_manager = self.futures_rate_manager
            else:  # SPOT
                rate_manager = self.spot_rate_manager

            rate_manager.wait_before_request()

            # æ ¹æ®klines_typeé€‰æ‹©API
            if klines_type == HistoricalKlinesType.FUTURES:
                klines = self.client.futures_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )
            else:  # SPOT
                klines = self.client.get_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…çš„æ•°æ®")
                rate_manager.handle_success()
                return []

            # è½¬æ¢ä¸ºKlineMarketTickerå¯¹è±¡
            result = [
                KlineMarketTicker(
                    symbol=symbol,
                    last_price=Decimal(str(kline[4])),  # æ”¶ç›˜ä»·ä½œä¸ºæœ€æ–°ä»·æ ¼
                    open_price=Decimal(str(kline[1])),
                    high_price=Decimal(str(kline[2])),
                    low_price=Decimal(str(kline[3])),
                    volume=Decimal(str(kline[5])),
                    close_time=kline[6],
                )
                for kline in data
            ]

            rate_manager.handle_success()
            return result

        except Exception as e:
            logger.error(f"[red]Error getting historical data for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get historical data: {e}") from e

    def _fetch_symbol_data(
        self,
        symbol: str,
        start_ts: str,
        end_ts: str,
        interval: Freq,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.FUTURES,
        retry_config: Optional[RetryConfig] = None,
        rate_limit_manager: Optional[RateLimitManager] = None,
    ) -> list[PerpetualMarketTicker]:
        """è·å–å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ® (å¢å¼ºç‰ˆ).

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_ts: å¼€å§‹æ—¶é—´æˆ³ (æ¯«ç§’)
            end_ts: ç»“æŸæ—¶é—´æˆ³ (æ¯«ç§’)
            interval: æ—¶é—´é—´éš”
            klines_type: è¡Œæƒ…ç±»å‹
            retry_config: é‡è¯•é…ç½®
            rate_limit_manager: é¢‘ç‡é™åˆ¶ç®¡ç†å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„self.rate_limit_manager
        """
        if retry_config is None:
            retry_config = RetryConfig()

        if rate_limit_manager is None:
            rate_limit_manager = self.rate_limit_manager

        backoff = ExponentialBackoff(retry_config)
        error_handler = EnhancedErrorHandler()

        while True:
            try:
                # æ•°æ®é¢„æ£€æŸ¥
                if start_ts and end_ts:
                    start_date = datetime.fromtimestamp(int(start_ts) / 1000).strftime("%Y-%m-%d")
                    logger.debug(f"è·å– {symbol} æ•°æ®: {start_date} ({start_ts} - {end_ts})")

                # é¢‘ç‡é™åˆ¶æ§åˆ¶ - åœ¨APIè°ƒç”¨å‰ç­‰å¾…
                rate_limit_manager.wait_before_request()

                # APIè°ƒç”¨
                klines = self.client.get_historical_klines_generator(
                    symbol=symbol,
                    interval=interval.value,
                    start_str=start_ts,
                    end_str=end_ts,
                    limit=1500,
                    klines_type=HistoricalKlinesType.to_binance(klines_type),
                )

                data = list(klines)
                if not data:
                    logger.debug(f"äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ— æ•°æ®")
                    rate_limit_manager.handle_success()
                    return []

                # æ•°æ®è´¨é‡æ£€æŸ¥
                valid_data = self._validate_kline_data(data, symbol)

                # è½¬æ¢ä¸ºå¯¹è±¡
                result = [
                    PerpetualMarketTicker(
                        symbol=symbol,
                        open_time=kline[0],
                        raw_data=kline,
                    )
                    for kline in valid_data
                ]

                logger.debug(f"æˆåŠŸè·å– {symbol}: {len(result)} æ¡è®°å½•")
                rate_limit_manager.handle_success()
                return result

            except Exception as e:
                severity = error_handler.classify_error(e)

                # ç‰¹æ®Šå¤„ç†é¢‘ç‡é™åˆ¶é”™è¯¯
                if error_handler.is_rate_limit_error(e):
                    wait_time = rate_limit_manager.handle_rate_limit_error()
                    logger.warning(f"ğŸš« é¢‘ç‡é™åˆ¶ - {symbol}ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•")
                    time.sleep(wait_time)
                    # é‡ç½®é€€é¿è®¡æ•°ï¼Œå› ä¸ºè¿™ä¸æ˜¯çœŸæ­£çš„"é”™è¯¯"
                    backoff.reset()
                    continue

                # å¤„ç†ä¸å¯é‡è¯•çš„é”™è¯¯
                if severity == ErrorSeverity.CRITICAL:
                    logger.error(f"âŒ è‡´å‘½é”™è¯¯ - {symbol}: {e}")
                    logger.error(f"å»ºè®®: {error_handler.get_recommended_action(e)}")
                    raise e

                if "Invalid symbol" in str(e):
                    logger.warning(f"âš ï¸ æ— æ•ˆäº¤æ˜“å¯¹: {symbol}")
                    raise InvalidSymbolError(f"æ— æ•ˆçš„äº¤æ˜“å¯¹: {symbol}") from e

                # åˆ¤æ–­æ˜¯å¦é‡è¯•
                if not error_handler.should_retry(e, backoff.attempt, retry_config.max_retries):
                    logger.error(f"âŒ é‡è¯•å¤±è´¥ - {symbol}: {e}")
                    if severity == ErrorSeverity.LOW:
                        # å¯¹äºä½ä¸¥é‡æ€§é”™è¯¯ï¼Œè¿”å›ç©ºç»“æœè€Œä¸æŠ›å‡ºå¼‚å¸¸
                        return []
                    raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: {e}") from e

                # æ‰§è¡Œé‡è¯•
                logger.warning(f"ğŸ”„ é‡è¯• {backoff.attempt + 1}/{retry_config.max_retries} - {symbol}: {e}")
                logger.info(f"ğŸ’¡ å»ºè®®: {error_handler.get_recommended_action(e)}")

                try:
                    backoff.wait()
                except Exception:
                    logger.error(f"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° - {symbol}")
                    raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°") from e

    def _validate_kline_data(self, data: List, symbol: str) -> List:
        """éªŒè¯Kçº¿æ•°æ®è´¨é‡"""
        if not data:
            return data

        valid_data = []
        issues = []

        for i, kline in enumerate(data):
            try:
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if len(kline) < 8:
                    issues.append(f"è®°å½•{i}: æ•°æ®å­—æ®µä¸è¶³")
                    continue

                # æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§
                open_price = float(kline[1])
                high_price = float(kline[2])
                low_price = float(kline[3])
                close_price = float(kline[4])
                volume = float(kline[5])

                # åŸºç¡€é€»è¾‘æ£€æŸ¥
                if high_price < max(open_price, close_price, low_price):
                    issues.append(f"è®°å½•{i}: æœ€é«˜ä»·å¼‚å¸¸")
                    continue

                if low_price > min(open_price, close_price, high_price):
                    issues.append(f"è®°å½•{i}: æœ€ä½ä»·å¼‚å¸¸")
                    continue

                if volume < 0:
                    issues.append(f"è®°å½•{i}: æˆäº¤é‡ä¸ºè´Ÿ")
                    continue

                valid_data.append(kline)

            except (ValueError, IndexError) as e:
                issues.append(f"è®°å½•{i}: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
                continue

        if issues:
            issue_count = len(issues)
            total_count = len(data)
            if issue_count > total_count * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æœ‰é—®é¢˜
                logger.warning(f"âš ï¸ {symbol} æ•°æ®è´¨é‡é—®é¢˜: {issue_count}/{total_count} æ¡è®°å½•å¼‚å¸¸")
                for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    logger.debug(f"   - {issue}")
                if len(issues) > 5:
                    logger.debug(f"   - ... è¿˜æœ‰ {len(issues) - 5} ä¸ªé—®é¢˜")

        return valid_data

    def _create_integrity_report(
        self,
        symbols: List[str],
        successful_symbols: List[str],
        failed_symbols: List[str],
        missing_periods: List[Dict[str, str]],
        start_time: str,
        end_time: str,
        interval: Freq,
        db_file_path: Path,
    ) -> IntegrityReport:
        """åˆ›å»ºæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š"""
        try:
            if not self.db:
                raise ValueError("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")

            logger.info("ğŸ” æ‰§è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")

            # è®¡ç®—åŸºç¡€æŒ‡æ ‡
            total_symbols = len(symbols)
            success_count = len(successful_symbols)
            basic_quality_score = success_count / total_symbols if total_symbols > 0 else 0

            recommendations = []
            detailed_issues = []

            # æ£€æŸ¥æˆåŠŸä¸‹è½½çš„æ•°æ®è´¨é‡ï¼ˆå¯¹äºæµ‹è¯•æ•°æ®é‡‡ç”¨å®½æ¾ç­–ç•¥ï¼‰
            quality_issues = 0
            sample_symbols = successful_symbols[: min(5, len(successful_symbols))]  # å‡å°‘æŠ½æ ·æ•°é‡

            # å¦‚æœæ˜¯å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡å®Œæ•´æ€§æ£€æŸ¥
            if start_time == end_time:
                logger.debug("æ£€æµ‹åˆ°å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯¦ç»†å®Œæ•´æ€§æ£€æŸ¥")
                sample_symbols = []

            for symbol in sample_symbols:
                try:
                    # è¯»å–æ•°æ®è¿›è¡Œè´¨é‡æ£€æŸ¥
                    # ç¡®ä¿æ—¶é—´æ ¼å¼æ­£ç¡®
                    check_start_time = pd.to_datetime(start_time).strftime("%Y-%m-%d")
                    check_end_time = pd.to_datetime(end_time).strftime("%Y-%m-%d")

                    df = self.db.read_data(
                        start_time=check_start_time,
                        end_time=check_end_time,
                        freq=interval,
                        symbols=[symbol],
                        raise_on_empty=False,
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
                        symbol_data = (
                            df.loc[symbol] if symbol in df.index.get_level_values("symbol") else pd.DataFrame()
                        )
                        if not symbol_data.empty:
                            # è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                            time_diff = pd.to_datetime(check_end_time) - pd.to_datetime(check_start_time)
                            expected_points = self._calculate_expected_data_points(time_diff, interval)
                            actual_points = len(symbol_data)

                            completeness = actual_points / expected_points if expected_points > 0 else 0
                            if completeness < 0.8:  # å°‘äº80%è®¤ä¸ºæœ‰é—®é¢˜
                                quality_issues += 1
                                detailed_issues.append(
                                    f"{symbol}: æ•°æ®å®Œæ•´æ€§{completeness:.1%} ({actual_points}/{expected_points})"
                                )
                    else:
                        quality_issues += 1
                        detailed_issues.append(f"{symbol}: æ— æ³•è¯»å–å·²ä¸‹è½½çš„æ•°æ®")

                except Exception as e:
                    quality_issues += 1
                    detailed_issues.append(f"{symbol}: æ£€æŸ¥å¤±è´¥ - {e}")

            # è°ƒæ•´è´¨é‡åˆ†æ•°
            if successful_symbols:
                sample_size = min(10, len(successful_symbols))
                quality_penalty = (quality_issues / sample_size) * 0.3  # æœ€å¤šå‡å°‘30%åˆ†æ•°
                final_quality_score = max(0, basic_quality_score - quality_penalty)
            else:
                final_quality_score = 0

            # ç”Ÿæˆå»ºè®®
            if final_quality_score < 0.5:
                recommendations.append("ğŸš¨ æ•°æ®è´¨é‡ä¸¥é‡ä¸è¶³ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
            elif final_quality_score < 0.8:
                recommendations.append("âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„äº¤æ˜“å¯¹")
            else:
                recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½")

            if failed_symbols:
                recommendations.append(f"ğŸ“ {len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå»ºè®®å•ç‹¬é‡è¯•")
                if len(failed_symbols) <= 5:
                    recommendations.append(f"å¤±è´¥äº¤æ˜“å¯¹: {', '.join(failed_symbols)}")

            if quality_issues > 0:
                recommendations.append(f"âš ï¸ å‘ç°{quality_issues}ä¸ªæ•°æ®è´¨é‡é—®é¢˜")
                recommendations.extend(detailed_issues[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜

            # ç½‘ç»œå’ŒAPIå»ºè®®
            if len(failed_symbols) > total_symbols * 0.3:
                recommendations.append("ğŸŒ å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé™åˆ¶")

            logger.info(f"âœ… å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ: è´¨é‡åˆ†æ•° {final_quality_score:.1%}")

            return IntegrityReport(
                total_symbols=total_symbols,
                successful_symbols=success_count,
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=final_quality_score,
                recommendations=recommendations,
            )

        except Exception as e:
            logger.warning(f"âš ï¸ å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æŠ¥å‘Š
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=len(successful_symbols),
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=(len(successful_symbols) / len(symbols) if symbols else 0),
                recommendations=[f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}", "å»ºè®®æ‰‹åŠ¨éªŒè¯æ•°æ®è´¨é‡"],
            )

    @calculation_cache
    def _calculate_expected_data_points(self, time_diff: timedelta, interval: Freq) -> int:
        """è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡"""
        return TimeUtils.calculate_expected_data_points(time_diff, interval)

    def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        output_path: Path | str,
        top_k: int | None = None,
        top_ratio: float | None = None,
        description: str | None = None,
        delay_days: int = 7,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶.

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            t1_months: T1æ—¶é—´çª—å£ï¼ˆæœˆï¼‰ï¼Œç”¨äºè®¡ç®—mean daily amount
            t2_months: T2æ»šåŠ¨é¢‘ç‡ï¼ˆæœˆï¼‰ï¼Œuniverseé‡æ–°é€‰æ‹©çš„é¢‘ç‡
            t3_months: T3åˆçº¦æœ€å°åˆ›å»ºæ—¶é—´ï¼ˆæœˆï¼‰ï¼Œç”¨äºç­›é™¤æ–°åˆçº¦
            output_path: universeè¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            top_k: é€‰å–çš„topåˆçº¦æ•°é‡ (ä¸ top_ratio äºŒé€‰ä¸€)
            top_ratio: é€‰å–çš„topåˆçº¦æ¯”ç‡ (ä¸ top_k äºŒé€‰ä¸€)
            description: æè¿°ä¿¡æ¯
            delay_days: åœ¨é‡æ–°å¹³è¡¡æ—¥æœŸå‰é¢å¤–å¾€å‰æ¨çš„å¤©æ•°ï¼Œé»˜è®¤7å¤©
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤1.0ç§’
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤3.0ç§’
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤5ä¸ª
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªç­›é€‰ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            UniverseDefinition: å®šä¹‰çš„universe
        """
        try:
            # éªŒè¯å¹¶å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_path_obj = self._validate_and_prepare_path(
                output_path,
                is_file=True,
                file_name=(
                    f"universe_{start_date}_{end_date}_{t1_months}_{t2_months}_{t3_months}_{top_k or top_ratio}.json"
                ),
            )

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            start_date = self._standardize_date_format(start_date)
            end_date = self._standardize_date_format(end_date)

            # åˆ›å»ºé…ç½®
            config = UniverseConfig(
                start_date=start_date,
                end_date=end_date,
                t1_months=t1_months,
                t2_months=t2_months,
                t3_months=t3_months,
                delay_days=delay_days,
                quote_asset=quote_asset,
                top_k=top_k,
                top_ratio=top_ratio,
            )

            logger.info(f"å¼€å§‹å®šä¹‰universe: {start_date} åˆ° {end_date}")
            log_selection_criteria = f"Top-K={top_k}" if top_k else f"Top-Ratio={top_ratio}"
            logger.info(f"å‚æ•°: T1={t1_months}æœˆ, T2={t2_months}æœˆ, T3={t3_months}æœˆ, {log_selection_criteria}")

            # ç”Ÿæˆé‡æ–°é€‰æ‹©æ—¥æœŸåºåˆ— (æ¯T2ä¸ªæœˆ)
            # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹
            rebalance_dates = self._generate_rebalance_dates(start_date, end_date, t2_months)

            logger.info("é‡å¹³è¡¡è®¡åˆ’:")
            logger.info(f"  - æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            logger.info(f"  - é‡å¹³è¡¡é—´éš”: æ¯{t2_months}ä¸ªæœˆ")
            logger.info(f"  - æ•°æ®å»¶è¿Ÿ: {delay_days}å¤©")
            logger.info(f"  - T1æ•°æ®çª—å£: {t1_months}ä¸ªæœˆ")
            logger.info(f"  - é‡å¹³è¡¡æ—¥æœŸ: {rebalance_dates}")

            if not rebalance_dates:
                raise ValueError("æ— æ³•ç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´å’ŒT2å‚æ•°")

            # æ”¶é›†æ‰€æœ‰å‘¨æœŸçš„snapshots
            all_snapshots = []

            # åœ¨æ¯ä¸ªé‡æ–°é€‰æ‹©æ—¥æœŸè®¡ç®—universe
            for i, rebalance_date in enumerate(rebalance_dates):
                logger.info(f"å¤„ç†æ—¥æœŸ {i + 1}/{len(rebalance_dates)}: {rebalance_date}")

                # è®¡ç®—åŸºå‡†æ—¥æœŸï¼ˆé‡æ–°å¹³è¡¡æ—¥æœŸå‰delay_dayså¤©ï¼‰
                base_date = pd.to_datetime(rebalance_date) - timedelta(days=delay_days)
                calculated_t1_end = base_date.strftime("%Y-%m-%d")

                # è®¡ç®—T1å›çœ‹æœŸé—´çš„å¼€å§‹æ—¥æœŸï¼ˆä»base_dateå¾€å‰æ¨T1ä¸ªæœˆï¼‰
                calculated_t1_start = self._subtract_months(calculated_t1_end, t1_months)

                logger.info(
                    f"å‘¨æœŸ {i + 1}: åŸºå‡†æ—¥æœŸ={calculated_t1_end} (é‡æ–°å¹³è¡¡æ—¥æœŸå‰{delay_days}å¤©), "
                    f"T1æ•°æ®æœŸé—´={calculated_t1_start} åˆ° {calculated_t1_end}"
                )

                # è·å–ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹å’Œå®ƒä»¬çš„mean daily amount
                universe_symbols, mean_amounts = self._calculate_universe_for_date(
                    calculated_t1_start,
                    calculated_t1_end,
                    t3_months=t3_months,
                    top_k=top_k,
                    top_ratio=top_ratio,
                    api_delay_seconds=api_delay_seconds,
                    batch_delay_seconds=batch_delay_seconds,
                    batch_size=batch_size,
                    quote_asset=quote_asset,
                )

                # åˆ›å»ºè¯¥å‘¨æœŸçš„snapshot
                snapshot = UniverseSnapshot.create_with_dates_and_timestamps(
                    usage_t1_start=rebalance_date,  # å®é™…ä½¿ç”¨å¼€å§‹æ—¥æœŸ
                    usage_t1_end=min(
                        end_date,
                        (pd.to_datetime(rebalance_date) + pd.DateOffset(months=t1_months)).strftime("%Y-%m-%d"),
                    ),  # å®é™…ä½¿ç”¨ç»“æŸæ—¥æœŸ
                    calculated_t1_start=calculated_t1_start,  # è®¡ç®—å‘¨æœŸå¼€å§‹æ—¥æœŸ
                    calculated_t1_end=calculated_t1_end,  # è®¡ç®—å‘¨æœŸç»“æŸæ—¥æœŸï¼ˆåŸºå‡†æ—¥æœŸï¼‰
                    symbols=universe_symbols,
                    mean_daily_amounts=mean_amounts,
                    metadata={
                        "calculated_t1_start": calculated_t1_start,
                        "calculated_t1_end": calculated_t1_end,
                        "delay_days": delay_days,
                        "quote_asset": quote_asset,
                        "selected_symbols_count": len(universe_symbols),
                    },
                )

                all_snapshots.append(snapshot)

                logger.info(f"âœ… æ—¥æœŸ {rebalance_date}: é€‰æ‹©äº† {len(universe_symbols)} ä¸ªäº¤æ˜“å¯¹")

            # åˆ›å»ºå®Œæ•´çš„universeå®šä¹‰
            universe_def = UniverseDefinition(
                config=config,
                snapshots=all_snapshots,
                creation_time=datetime.now(),
                description=description,
            )

            # ä¿å­˜æ±‡æ€»çš„universeå®šä¹‰
            universe_def.save_to_file(output_path_obj)

            logger.info("ğŸ‰ Universeå®šä¹‰å®Œæˆï¼")
            logger.info(f"ğŸ“ åŒ…å« {len(all_snapshots)} ä¸ªé‡æ–°å¹³è¡¡å‘¨æœŸ")
            logger.info(f"ğŸ“‹ æ±‡æ€»æ–‡ä»¶: {output_path_obj}")

            return universe_def

        except Exception as e:
            logger.error(f"[red]å®šä¹‰universeå¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"å®šä¹‰universeå¤±è´¥: {e}") from e

    @time_cache
    def _standardize_date_format(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDã€‚"""
        return TimeUtils.standardize_date_format(date_str)

    @calculation_cache
    def _generate_rebalance_dates(self, start_date: str, end_date: str, t2_months: int) -> list[str]:
        """ç”Ÿæˆé‡æ–°é€‰æ‹©universeçš„æ—¥æœŸåºåˆ—ã€‚

        ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¿™äº›æ—¥æœŸè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            t2_months: é‡æ–°å¹³è¡¡é—´éš”ï¼ˆæœˆï¼‰

        Returns:
            list[str]: é‡å¹³è¡¡æ—¥æœŸåˆ—è¡¨
        """
        return TimeUtils.generate_rebalance_dates(start_date, end_date, t2_months)

    @time_cache
    def _subtract_months(self, date_str: str, months: int) -> str:
        """ä»æ—¥æœŸå‡å»æŒ‡å®šæœˆæ•°ã€‚"""
        return TimeUtils.subtract_months(date_str, months)

    def _get_available_symbols_for_period(self, start_date: str, end_date: str, quote_asset: str = "USDT") -> list[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µå†…å®é™…å¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹

        Returns:
            list[str]: åœ¨è¯¥æ—¶é—´æ®µå†…æœ‰æ•°æ®çš„äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            # å…ˆè·å–å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦ä½œä¸ºå€™é€‰ï¼ˆç­›é€‰æŒ‡å®šçš„åŸºå‡†èµ„äº§ï¼‰
            candidate_symbols = self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)
            logger.info(
                f"æ£€æŸ¥ {len(candidate_symbols)} ä¸ª{quote_asset}å€™é€‰äº¤æ˜“å¯¹åœ¨ {start_date} åˆ° {end_date} æœŸé—´çš„å¯ç”¨æ€§..."
            )

            available_symbols = []
            logger.start_download_progress(len(candidate_symbols), "æ£€æŸ¥äº¤æ˜“å¯¹å¯ç”¨æ€§")
            for i, symbol in enumerate(candidate_symbols):
                # æ£€æŸ¥åœ¨èµ·å§‹æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®
                if self.check_symbol_exists_on_date(symbol, start_date):
                    available_symbols.append(symbol)
                if i % 10 == 0 or i == len(candidate_symbols) - 1:
                    logger.update_download_progress(
                        f"å·²æ£€æŸ¥ {i + 1}/{len(candidate_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œæ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹"
                    )
            logger.info(
                f"åœ¨ {start_date} åˆ° {end_date} æœŸé—´æ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨çš„{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹"
            )
            logger.stop_download_progress()
            return available_symbols

        except Exception as e:
            logger.warning(f"è·å–å¯ç”¨äº¤æ˜“å¯¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœAPIæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦
            return self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)

    def _calculate_universe_for_date(
        self,
        calculated_t1_start: str,
        calculated_t1_end: str,
        t3_months: int,
        top_k: int | None = None,
        top_ratio: float | None = None,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> tuple[list[str], dict[str, float]]:
        """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„universeã€‚

        Args:
            rebalance_date: é‡å¹³è¡¡æ—¥æœŸ
            t1_start_date: T1å¼€å§‹æ—¥æœŸ
            t3_months: T3æœˆæ•°
            top_k: é€‰æ‹©çš„topæ•°é‡
            top_ratio: é€‰æ‹©çš„topæ¯”ç‡
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹
        """
        try:
            # è·å–åœ¨è¯¥æ—¶é—´æ®µå†…å®é™…å­˜åœ¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹
            actual_symbols = self._get_available_symbols_for_period(calculated_t1_start, calculated_t1_end, quote_asset)

            # ç­›é™¤æ–°åˆçº¦ (åˆ›å»ºæ—¶é—´ä¸è¶³T3ä¸ªæœˆçš„)
            logger.info(f"ç­›é™¤æ–°åˆçº¦ (åˆ›å»ºæ—¶é—´ä¸è¶³{t3_months}ä¸ªæœˆ)")
            cutoff_date = self._subtract_months(calculated_t1_end, t3_months)
            eligible_symbols = [
                symbol for symbol in actual_symbols if self._symbol_exists_before_date(symbol, cutoff_date)
            ]

            if not eligible_symbols:
                logger.warning(f"æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end}: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹")
                return [], {}

            # é€šè¿‡APIè·å–æ•°æ®è®¡ç®—mean daily amount
            mean_amounts = {}
            num_excluded = len(actual_symbols) - len(eligible_symbols)
            num_eligible = len(eligible_symbols)
            logger.info(f"å·²ç­›é™¤{num_excluded}ä¸ªåˆçº¦ï¼Œå¼€å§‹è·å– {num_eligible} ä¸ªäº¤æ˜“å¯¹çš„å†å²æ•°æ®")
            logger.start_download_progress(len(eligible_symbols), "å¼€å§‹é€šè¿‡ API è·å–å†å²æ•°æ®")

            # ä½¿ç”¨æ‰¹é‡æ“ä½œé¢‘ç‡ç®¡ç†å™¨ï¼Œè®¾ç½®åˆé€‚çš„å»¶è¿Ÿ
            universe_rate_manager = RateLimitManager(base_delay=api_delay_seconds)
            # ä¸ºuniverseè®¡ç®—ä½¿ç”¨æ‰¹é‡é¢‘ç‡ç®¡ç†å™¨çš„åŸºç¡€é…ç½®
            universe_rate_manager.max_requests_per_minute = self.batch_rate_manager.max_requests_per_minute

            for i, symbol in enumerate(eligible_symbols):
                try:
                    # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
                    start_ts = self._date_to_timestamp_start(calculated_t1_start)
                    end_ts = self._date_to_timestamp_end(calculated_t1_end, Freq.d1)

                    # è·å–å†å²Kçº¿æ•°æ®ï¼Œä½¿ç”¨ä¸“ç”¨çš„é¢‘ç‡ç®¡ç†å™¨
                    klines = self._fetch_symbol_data(
                        symbol=symbol,
                        start_ts=start_ts,
                        end_ts=end_ts,
                        interval=Freq.d1,
                        rate_limit_manager=universe_rate_manager,
                    )

                    if klines:
                        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                        expected_days = (
                            pd.to_datetime(calculated_t1_end) - pd.to_datetime(calculated_t1_start)
                        ).days + 1
                        actual_days = len(klines)

                        if actual_days < expected_days * 0.8:  # å…è®¸20%çš„æ•°æ®ç¼ºå¤±
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} æ•°æ®ä¸å®Œæ•´: æœŸæœ›{expected_days}å¤©ï¼Œå®é™…{actual_days}å¤©")

                        # è®¡ç®—å¹³å‡æ—¥æˆäº¤é¢
                        amounts = []
                        for kline in klines:
                            try:
                                # kline.raw_data[7] æ˜¯æˆäº¤é¢ï¼ˆUSDTï¼‰
                                if kline.raw_data and len(kline.raw_data) > 7:
                                    amount = float(kline.raw_data[7])
                                    amounts.append(amount)
                            except (ValueError, IndexError):
                                continue

                        if amounts:
                            mean_amount = sum(amounts) / len(amounts)
                            mean_amounts[symbol] = mean_amount
                        else:
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} åœ¨æœŸé—´å†…æ²¡æœ‰æœ‰æ•ˆçš„æˆäº¤é‡æ•°æ®")
                    if i % 10 == 0 or i == len(eligible_symbols) - 1:
                        logger.update_download_progress(f"å·²å¤„ç† {i + 1}/{len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹")
                    time.sleep(0.1)
                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    continue

            logger.stop_download_progress()

            # æŒ‰mean daily amountæ’åºå¹¶é€‰æ‹©top_kæˆ–top_ratio
            if mean_amounts:
                sorted_symbols = sorted(mean_amounts.items(), key=lambda x: x[1], reverse=True)

                if top_ratio is not None:
                    num_to_select = int(len(sorted_symbols) * top_ratio)
                elif top_k is not None:
                    num_to_select = top_k
                else:
                    # é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æä¾›top_kæˆ–top_ratioï¼Œåˆ™é€‰æ‹©æ‰€æœ‰
                    num_to_select = len(sorted_symbols)

                top_symbols = sorted_symbols[:num_to_select]

                universe_symbols = [symbol for symbol, _ in top_symbols]
                final_amounts = dict(top_symbols)

                # æ˜¾ç¤ºé€‰æ‹©ç»“æœ
                if len(universe_symbols) <= 10:
                    logger.info(f"é€‰ä¸­çš„äº¤æ˜“å¯¹: {universe_symbols}")
                else:
                    logger.info(f"Top 5: {universe_symbols[:5]}")
                    logger.info("å®Œæ•´åˆ—è¡¨å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
            else:
                # å¦‚æœæ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¿”å›ç©º
                universe_symbols = []
                final_amounts = {}
                logger.warning("æ— æ³•é€šè¿‡APIè·å–æ•°æ®ï¼Œè¿”å›ç©ºçš„universe")

            return universe_symbols, final_amounts

        except Exception as e:
            logger.error(f"è®¡ç®—æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end} çš„universeæ—¶å‡ºé”™: {e}")
            return [], {}

    @symbol_cache
    def _symbol_exists_before_date(self, symbol: str, cutoff_date: str) -> bool:
        """æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å‰å°±å­˜åœ¨ã€‚"""
        try:
            # æ£€æŸ¥åœ¨cutoff_dateä¹‹å‰æ˜¯å¦æœ‰æ•°æ®
            # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥cutoff_dateå‰ä¸€å¤©çš„æ•°æ®
            check_date = (pd.to_datetime(cutoff_date) - timedelta(days=1)).strftime("%Y-%m-%d")
            return self.check_symbol_exists_on_date(symbol, check_date)
        except Exception:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºå­˜åœ¨
            return True

    def download_universe_data(
        self,
        universe_file: Path | str,
        db_path: Path | str,
        data_path: Path | str | None = None,
        interval: Freq = Freq.m1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
        request_delay: float = 0.5,  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    ) -> None:
        """æŒ‰å‘¨æœŸåˆ†åˆ«ä¸‹è½½universeæ•°æ®ï¼ˆæ›´ç²¾ç¡®çš„ä¸‹è½½æ–¹å¼ï¼‰ã€‚

        è¿™ç§æ–¹å¼ä¸ºæ¯ä¸ªé‡å¹³è¡¡å‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®ï¼Œå¯ä»¥é¿å…ä¸‹è½½ä¸å¿…è¦çš„æ•°æ®ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            data_path: æ•°æ®æ–‡ä»¶å­˜å‚¨è·¯å¾„ (å¯é€‰ï¼Œç”¨äºå­˜å‚¨å…¶ä»–æ•°æ®æ–‡ä»¶)
            interval: æ•°æ®é¢‘ç‡
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: ç¼“å†²å¤©æ•°
            request_delay: æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’
        """
        try:
            # éªŒè¯è·¯å¾„
            universe_file_obj = self._validate_and_prepare_path(universe_file, is_file=True)
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)

            # data_pathæ˜¯å¯é€‰çš„ï¼Œå¦‚æœæä¾›åˆ™éªŒè¯
            data_path_obj = None
            if data_path:
                data_path_obj = self._validate_and_prepare_path(data_path, is_file=False)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            logger.info("ğŸ“Š æŒ‰å‘¨æœŸä¸‹è½½æ•°æ®:")
            logger.info(f"   - æ€»å¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")
            logger.info(f"   - è¯·æ±‚é—´éš”: {request_delay}ç§’")
            logger.info(f"   - æ•°æ®åº“è·¯å¾„: {db_file_path}")
            if data_path_obj:
                logger.info(f"   - æ•°æ®æ–‡ä»¶è·¯å¾„: {data_path_obj}")

            # ä¸ºæ¯ä¸ªå‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(f"ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}")

                logger.info(f"   - äº¤æ˜“å¯¹æ•°é‡: {len(snapshot.symbols)}")
                logger.info(
                    f"   - è®¡ç®—æœŸé—´: {snapshot.calculated_t1_start} åˆ° {snapshot.calculated_t1_end} (å®šä¹‰universe)"
                )
                logger.info(f"   - ä½¿ç”¨æœŸé—´: {snapshot.start_date} åˆ° {snapshot.end_date} (å®é™…ä½¿ç”¨)")
                logger.info(
                    f"   - ä¸‹è½½èŒƒå›´: {snapshot.start_date} åˆ° {snapshot.end_date} (å«{include_buffer_days}å¤©ç¼“å†²)"
                )

                # ä¸‹è½½è¯¥å‘¨æœŸçš„ä½¿ç”¨æœŸé—´æ•°æ®
                self.get_perpetual_data(
                    symbols=snapshot.symbols,
                    start_time=snapshot.start_date,
                    end_time=snapshot.end_date,
                    db_path=db_file_path,
                    interval=interval,
                    max_workers=max_workers,
                    max_retries=max_retries,
                    enable_integrity_check=True,
                    request_delay=request_delay,
                )

                logger.info(f"   âœ… å¿«ç…§ {snapshot.effective_date} ä¸‹è½½å®Œæˆ")

            logger.info("ğŸ‰ æ‰€æœ‰universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

        except Exception as e:
            logger.error(f"[red]æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e

    def _analyze_universe_data_requirements(
        self,
        universe_def: UniverseDefinition,
        buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> dict[str, Any]:
        """åˆ†æuniverseæ•°æ®ä¸‹è½½éœ€æ±‚ã€‚

        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•è®¡ç®—æ€»ä½“èŒƒå›´ï¼Œä½†å®é™…ä¸‹è½½åº”è¯¥ä½¿ç”¨å„å¿«ç…§çš„ä½¿ç”¨æœŸé—´ã€‚
        æ¨èä½¿ç”¨ download_universe_data_by_periods æ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            buffer_days: ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Returns:
            Dict: ä¸‹è½½è®¡åˆ’è¯¦æƒ…
        """
        import pandas as pd

        # æ”¶é›†æ‰€æœ‰çš„äº¤æ˜“å¯¹å’Œå®é™…ä½¿ç”¨æ—¶é—´èŒƒå›´
        all_symbols = set()
        usage_dates = []  # ä½¿ç”¨æœŸé—´çš„æ—¥æœŸ
        calculation_dates = []  # è®¡ç®—æœŸé—´çš„æ—¥æœŸ

        for snapshot in universe_def.snapshots:
            all_symbols.update(snapshot.symbols)

            # ä½¿ç”¨æœŸé—´ - å®é™…éœ€è¦ä¸‹è½½çš„æ•°æ®
            usage_dates.extend(
                [
                    snapshot.start_date,  # å®é™…ä½¿ç”¨å¼€å§‹
                    snapshot.end_date,  # å®é™…ä½¿ç”¨ç»“æŸ
                ]
            )

            # è®¡ç®—æœŸé—´ - ç”¨äºå®šä¹‰universeçš„æ•°æ®
            calculation_dates.extend(
                [
                    snapshot.calculated_t1_start,
                    snapshot.calculated_t1_end,
                    snapshot.effective_date,
                ]
            )

        # è®¡ç®—æ€»ä½“æ—¶é—´èŒƒå›´ - åŸºäºä½¿ç”¨æœŸé—´è€Œä¸æ˜¯è®¡ç®—æœŸé—´
        start_date = pd.to_datetime(min(usage_dates)) - timedelta(days=buffer_days)
        end_date = pd.to_datetime(max(usage_dates)) + timedelta(days=buffer_days)

        if extend_to_present:
            end_date = max(end_date, pd.to_datetime("today"))

        # æ·»åŠ æ›´å¤šè¯¦ç»†ä¿¡æ¯
        return {
            "unique_symbols": sorted(all_symbols),
            "total_symbols": len(all_symbols),
            "overall_start_date": start_date.strftime("%Y-%m-%d"),
            "overall_end_date": end_date.strftime("%Y-%m-%d"),
            "usage_period_start": pd.to_datetime(min(usage_dates)).strftime("%Y-%m-%d"),
            "usage_period_end": pd.to_datetime(max(usage_dates)).strftime("%Y-%m-%d"),
            "calculation_period_start": pd.to_datetime(min(calculation_dates)).strftime("%Y-%m-%d"),
            "calculation_period_end": pd.to_datetime(max(calculation_dates)).strftime("%Y-%m-%d"),
            "snapshots_count": len(universe_def.snapshots),
            "note": "æ¨èä½¿ç”¨download_universe_data_by_periodsæ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½",
        }

    def _verify_universe_data_integrity(
        self,
        universe_def: UniverseDefinition,
        db_path: Path,
        interval: Freq,
        download_plan: dict[str, Any],
    ) -> None:
        """éªŒè¯ä¸‹è½½çš„universeæ•°æ®å®Œæ•´æ€§ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            interval: æ•°æ®é¢‘ç‡
            download_plan: ä¸‹è½½è®¡åˆ’
        """
        try:
            from cryptoservice.data import MarketDB

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ - ç›´æ¥ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db = MarketDB(str(db_path))

            logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            incomplete_symbols: list[str] = []
            missing_data: list[dict[str, str]] = []
            successful_snapshots = 0

            for snapshot in universe_def.snapshots:
                try:
                    # æ£€æŸ¥è¯¥å¿«ç…§çš„ä¸»è¦äº¤æ˜“å¯¹æ•°æ®ï¼ŒåŸºäºä½¿ç”¨æœŸé—´éªŒè¯
                    # ä½¿ç”¨æ‰©å±•çš„æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿èƒ½å¤Ÿæ‰¾åˆ°æ•°æ®
                    usage_start = pd.to_datetime(snapshot.start_date) - timedelta(days=3)
                    usage_end = pd.to_datetime(snapshot.end_date) + timedelta(days=3)

                    df = db.read_data(
                        symbols=snapshot.symbols[:3],  # åªæ£€æŸ¥å‰3ä¸ªä¸»è¦äº¤æ˜“å¯¹
                        start_time=usage_start.strftime("%Y-%m-%d"),
                        end_time=usage_end.strftime("%Y-%m-%d"),
                        freq=interval,
                        raise_on_empty=False,  # ä¸åœ¨æ²¡æœ‰æ•°æ®æ—¶æŠ›å‡ºå¼‚å¸¸
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¦†ç›–çš„äº¤æ˜“å¯¹æ•°é‡
                        available_symbols = df.index.get_level_values("symbol").unique()
                        missing_symbols = set(snapshot.symbols[:3]) - set(available_symbols)
                        if missing_symbols:
                            incomplete_symbols.extend(missing_symbols)
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date}ç¼ºå°‘äº¤æ˜“å¯¹: {list(missing_symbols)}")
                        else:
                            successful_snapshots += 1
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date} éªŒè¯æˆåŠŸ")
                    else:
                        logger.debug(f"å¿«ç…§ {snapshot.effective_date} åœ¨æ‰©å±•æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°æ•°æ®")
                        missing_data.append(
                            {
                                "snapshot_date": snapshot.effective_date,
                                "error": "No data in extended time range",
                            }
                        )

                except Exception as e:
                    logger.debug(f"éªŒè¯å¿«ç…§ {snapshot.effective_date} æ—¶å‡ºé”™: {e}")
                    # ä¸å†è®°å½•ä¸ºä¸¥é‡é”™è¯¯ï¼Œåªæ˜¯è®°å½•è°ƒè¯•ä¿¡æ¯
                    missing_data.append({"snapshot_date": snapshot.effective_date, "error": str(e)})

            # æŠ¥å‘ŠéªŒè¯ç»“æœ - æ›´å‹å¥½çš„æŠ¥å‘Šæ–¹å¼
            total_snapshots = len(universe_def.snapshots)
            success_rate = successful_snapshots / total_snapshots if total_snapshots > 0 else 0

            logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
            logger.info(f"   - å·²ä¸‹è½½äº¤æ˜“å¯¹: {download_plan['total_symbols']} ä¸ª")
            logger.info(f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° {download_plan['overall_end_date']}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - æˆåŠŸéªŒè¯å¿«ç…§: {successful_snapshots}/{total_snapshots} ({success_rate:.1%})")

            # åªæœ‰åœ¨æˆåŠŸç‡å¾ˆä½æ—¶æ‰ç»™å‡ºè­¦å‘Š
            if success_rate < 0.95:
                logger.warning(f"âš ï¸ éªŒè¯æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}")
                if incomplete_symbols:
                    unique_incomplete = set(incomplete_symbols)
                    logger.warning(f"   - æ•°æ®ä¸å®Œæ•´çš„äº¤æ˜“å¯¹: {len(unique_incomplete)} ä¸ª")
                    if len(unique_incomplete) <= 5:
                        logger.warning(f"   - å…·ä½“äº¤æ˜“å¯¹: {list(unique_incomplete)}")

                if missing_data:
                    logger.warning(f"   - æ— æ³•éªŒè¯çš„å¿«ç…§: {len(missing_data)} ä¸ª")
            else:
                logger.info("ğŸ“Š æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œåç»­åˆ†æ")

        except Exception as e:
            logger.warning(f"æ•°æ®å®Œæ•´æ€§éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œä½†ä¸å½±å“æ•°æ®ä½¿ç”¨: {e}")
            logger.info("ğŸ’¡ æç¤º: éªŒè¯å¤±è´¥ä¸ä»£è¡¨æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æŸ¥è¯¢å…·ä½“æ•°æ®è¿›è¡Œç¡®è®¤")

    def get_perpetual_data(
        self,
        symbols: list[str],
        start_time: str,
        db_path: Path | str,
        end_time: str | None = None,
        interval: Freq = Freq.h1,
        max_workers: int = 5,
        max_retries: int = 3,
        request_delay: float = 0.5,
        progress: Progress | None = None,
        completeness_threshold: float = 1.0,
        enable_integrity_check: bool = True,
    ) -> IntegrityReport:
        """è·å–æ°¸ç»­åˆçº¦æ•°æ®å¹¶å­˜å‚¨ï¼ˆåŸºäºæ•°æ®åº“çŠ¶æ€ç®¡ç†ï¼‰.

        åŸºäºæ•°æ®åº“çŠ¶æ€è‡ªåŠ¨å®ç°æ–­ç‚¹ç»­ä¼ ï¼Œæ— éœ€é¢å¤–çš„çŠ¶æ€æ–‡ä»¶ç®¡ç†ã€‚

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            end_time: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
            interval: æ—¶é—´é—´éš”
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            request_delay: è¯·æ±‚é—´éš”ç§’æ•°
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
            completeness_threshold: æ•°æ®å®Œæ•´æ€§é˜ˆå€¼ (0.95 = 95%)
            enable_integrity_check: æ˜¯å¦å¯ç”¨å®Œæ•´æ€§æ£€æŸ¥

        Returns:
            IntegrityReport: æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š
        """
        try:
            if not symbols:
                raise ValueError("Symbols list cannot be empty")

            # éªŒè¯å¹¶å‡†å¤‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)
            end_time = end_time or datetime.now().strftime("%Y-%m-%d")

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if self.db is None:
                self.db = MarketDB(str(db_file_path))

            # ä¸ºæ­¤æ¬¡ä¸‹è½½ä»»åŠ¡è®¾ç½®åˆé€‚çš„é¢‘ç‡é™åˆ¶ç®¡ç†å™¨
            # ä½¿ç”¨åˆçº¦é¢‘ç‡ç®¡ç†å™¨ï¼Œä½†è°ƒæ•´å»¶è¿Ÿä»¥é€‚åº”ç”¨æˆ·è®¾ç½®
            original_delay = self.futures_rate_manager.base_delay
            self.futures_rate_manager.base_delay = request_delay
            download_rate_manager = self.futures_rate_manager

            # è®¾ç½®ç²¾ç®€è¾“å‡ºæ¨¡å¼ä»¥å‡å°‘æ—¥å¿—å™ªéŸ³
            logger.set_output_mode(OutputMode.COMPACT)
            logger.info("ğŸš€ å¼€å§‹æ•°æ®ä¸‹è½½ä»»åŠ¡")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
            logger.info(f"ğŸ“Š äº¤æ˜“å¯¹æ•°é‡: {len(symbols)}")
            logger.info(f"â±ï¸ è¯·æ±‚é—´éš”: {request_delay}ç§’")
            logger.info(f"ğŸ¯ å®Œæ•´æ€§é˜ˆå€¼: {completeness_threshold:.1%}")

            # åŸºäºæ•°æ®åº“æ£€æŸ¥ç°æœ‰æ•°æ®
            need_download, already_complete = self._check_database_completeness(
                symbols, start_time, end_time, interval, completeness_threshold
            )

            logger.info("ğŸ“Š æ•°æ®çŠ¶æ€æ£€æŸ¥å®Œæˆ:")
            logger.info(f"   - éœ€è¦ä¸‹è½½: {len(need_download)} ä¸ª")
            logger.info(f"   - å·²å®Œæ•´: {len(already_complete)} ä¸ª")

            if not need_download:
                logger.success("âœ… æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½")
                return IntegrityReport(
                    total_symbols=len(symbols),
                    successful_symbols=len(already_complete),
                    failed_symbols=[],
                    missing_periods=[],
                    data_quality_score=1.0,
                    recommendations=["âœ… æ‰€æœ‰æ•°æ®å®Œæ•´ï¼Œæ— éœ€é¢å¤–æ“ä½œ"],
                )

            # è½¬æ¢æ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time)
            end_ts = self._date_to_timestamp_end(end_time, interval)

            # æ‰§è¡Œå¤šè½®ä¸‹è½½ï¼ˆä¸»è¦ä¸‹è½½ + é‡è¯•ï¼‰
            # å¯ç”¨è¿›åº¦æ¡æ˜¾ç¤º
            if not progress:
                logger.set_output_mode(OutputMode.NORMAL)
                logger.start_download_progress(len(need_download), "æ•°æ®ä¸‹è½½è¿›åº¦")

            download_results = self._execute_multi_round_download(
                need_download,
                start_ts,
                end_ts,
                interval,
                max_workers,
                max_retries,
                progress,
                download_rate_manager,
            )

            if not progress:
                logger.stop_download_progress()
                logger.set_output_mode(OutputMode.COMPACT)

            # ç»Ÿè®¡ç»“æœ
            successful_symbols = already_complete + [r["symbol"] for r in download_results if r["success"]]
            failed_symbols = [r["symbol"] for r in download_results if not r["success"]]
            total_records = sum(r.get("records", 0) for r in download_results)

            logger.success("ğŸ“Š ä¸‹è½½ä»»åŠ¡å®Œæˆç»Ÿè®¡:")
            logger.info(
                f"   âœ… æˆåŠŸ: {len(successful_symbols)}/{len(symbols)} ({len(successful_symbols) / len(symbols):.1%})"
            )
            if failed_symbols:
                logger.warning(f"   âŒ å¤±è´¥: {len(failed_symbols)} ä¸ª")
            logger.info(f"   ğŸ“ˆ æ–°å¢è®°å½•: {total_records:,} æ¡")
            logger.debug(f"   ğŸ’¾ æ•°æ®åº“: {db_file_path}")

            # ç”Ÿæˆå®Œæ•´æ€§æŠ¥å‘Š
            missing_periods = [
                {
                    "symbol": r["symbol"],
                    "period": f"{start_time} - {end_time}",
                    "reason": r.get("error", "Unknown error"),
                }
                for r in download_results
                if not r["success"]
            ]

            if enable_integrity_check:
                integrity_report = self._create_integrity_report(
                    symbols=symbols,
                    successful_symbols=successful_symbols,
                    failed_symbols=failed_symbols,
                    missing_periods=missing_periods,
                    start_time=start_time,
                    end_time=end_time,
                    interval=interval,
                    db_file_path=db_file_path,
                )
            else:
                data_quality_score = len(successful_symbols) / len(symbols) if symbols else 0
                recommendations = []
                if data_quality_score < 0.8:
                    recommendations.append("æ•°æ®æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå’ŒAPIé…ç½®")
                if failed_symbols:
                    recommendations.append(f"æœ‰{len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå¯é‡æ–°è¿è¡Œç»§ç»­ä¸‹è½½")

                integrity_report = IntegrityReport(
                    total_symbols=len(symbols),
                    successful_symbols=len(successful_symbols),
                    failed_symbols=failed_symbols,
                    missing_periods=missing_periods,
                    data_quality_score=data_quality_score,
                    recommendations=recommendations,
                )

            return integrity_report

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=0,
                failed_symbols=symbols,
                missing_periods=[
                    {
                        "symbol": s,
                        "period": f"{start_time} - {end_time or 'now'}",
                        "reason": str(e),
                    }
                    for s in symbols
                ],
                data_quality_score=0.0,
                recommendations=[
                    f"ä¸‹è½½å¤±è´¥: {e}",
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®",
                    "å¯é‡æ–°è¿è¡Œæ­¤æ–¹æ³•è¿›è¡Œé‡è¯•",
                ],
            )
        finally:
            # æ¢å¤åŸå§‹çš„å»¶è¿Ÿè®¾ç½®
            if "original_delay" in locals():
                self.futures_rate_manager.base_delay = original_delay

    def _check_database_completeness(
        self,
        symbols: list[str],
        start_time: str,
        end_time: str,
        interval: Freq,
        completeness_threshold: float = 0.95,
    ) -> tuple[list[str], list[str]]:
        """åŸºäºæ•°æ®åº“æ£€æŸ¥æ•°æ®å®Œæ•´æ€§.

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            interval: æ—¶é—´é—´éš”
            completeness_threshold: å®Œæ•´æ€§é˜ˆå€¼

        Returns:
            tuple: (éœ€è¦ä¸‹è½½çš„symbols, å·²å®Œæ•´çš„symbols)
        """
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®...")

        need_download = []
        already_complete = []

        # è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡
        start_dt = pd.to_datetime(start_time)
        end_dt = pd.to_datetime(end_time)
        time_diff = end_dt - start_dt
        expected_points = self._calculate_expected_data_points(time_diff, interval)

        for symbol in symbols:
            try:
                # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ•°æ®
                if self.db is None:
                    raise MarketDataFetchError("Database is not initialized")

                df = self.db.read_data(
                    symbols=[symbol],
                    start_time=start_time,
                    end_time=end_time,
                    freq=interval,
                    raise_on_empty=False,
                )

                if df is not None and not df.empty:
                    # è®¡ç®—æ•°æ®å®Œæ•´æ€§
                    actual_points = len(df)
                    completeness = actual_points / expected_points if expected_points > 0 else 0

                    if completeness >= completeness_threshold:
                        already_complete.append(symbol)
                        logger.debug(f"âœ… {symbol}: æ•°æ®å®Œæ•´ ({completeness:.1%}, {actual_points}/{expected_points})")
                    else:
                        need_download.append(symbol)
                        logger.debug(f"âš ï¸ {symbol}: æ•°æ®ä¸å®Œæ•´ ({completeness:.1%}, {actual_points}/{expected_points})")
                else:
                    need_download.append(symbol)
                    logger.debug(f"âŒ {symbol}: æ— æ•°æ®")

            except Exception as e:
                logger.debug(f"æ£€æŸ¥ {symbol} æ•°æ®æ—¶å‡ºé”™: {e}")
                need_download.append(symbol)

        return need_download, already_complete

    def _execute_multi_round_download(
        self,
        symbols: list[str],
        start_ts: str,
        end_ts: str,
        interval: Freq,
        max_workers: int,
        max_retries: int,
        progress: Progress | None = None,
        rate_limit_manager: Optional[RateLimitManager] = None,
    ) -> list[dict]:
        """æ‰§è¡Œå¤šè½®ä¸‹è½½ï¼ˆä¸»è¦ä¸‹è½½ + é‡è¯•å¤±è´¥çš„ï¼‰.

        Args:
            symbols: éœ€è¦ä¸‹è½½çš„äº¤æ˜“å¯¹åˆ—è¡¨
            start_ts: å¼€å§‹æ—¶é—´æˆ³
            end_ts: ç»“æŸæ—¶é—´æˆ³
            interval: æ—¶é—´é—´éš”
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
            rate_limit_manager: é¢‘ç‡é™åˆ¶ç®¡ç†å™¨

        Returns:
            list[dict]: ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from contextlib import nullcontext

        all_results = []
        symbols_to_process = symbols.copy()

        for round_num in range(max_retries + 1):
            if not symbols_to_process:
                break

            round_name = "ä¸»è¦ä¸‹è½½" if round_num == 0 else f"é‡è¯•ç¬¬{round_num}è½®"
            logger.info(f"ğŸ”„ å¼€å§‹{round_name}: {len(symbols_to_process)} ä¸ªäº¤æ˜“å¯¹")

            round_results = []

            with progress if progress is not None else nullcontext():
                task_desc = f"[cyan]{round_name}"
                overall_task = progress.add_task(task_desc, total=len(symbols_to_process)) if progress else None

                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [
                        executor.submit(
                            self._download_single_symbol,
                            symbol,
                            start_ts,
                            end_ts,
                            interval,
                            rate_limit_manager,
                        )
                        for symbol in symbols_to_process
                    ]

                    for future in as_completed(futures):
                        try:
                            result = future.result()
                            round_results.append(result)
                            all_results.append(result)

                            if progress and overall_task is not None:
                                progress.update(overall_task, advance=1)

                        except Exception as e:
                            logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {e}")

            # å‡†å¤‡ä¸‹ä¸€è½®çš„å¤±è´¥äº¤æ˜“å¯¹
            failed_symbols = [r["symbol"] for r in round_results if not r["success"]]
            successful_count = len(round_results) - len(failed_symbols)

            logger.info(f"ğŸ“Š {round_name}å®Œæˆ: æˆåŠŸ {successful_count}, å¤±è´¥ {len(failed_symbols)}")

            # æ›´æ–°å¾…å¤„ç†åˆ—è¡¨
            symbols_to_process = failed_symbols

        return all_results

    def _download_single_symbol(
        self,
        symbol: str,
        start_ts: str,
        end_ts: str,
        interval: Freq,
        rate_limit_manager: Optional[RateLimitManager] = None,
    ) -> dict:
        """ä¸‹è½½å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®.

        Args:
            symbol: äº¤æ˜“å¯¹
            start_ts: å¼€å§‹æ—¶é—´æˆ³
            end_ts: ç»“æŸæ—¶é—´æˆ³
            interval: æ—¶é—´é—´éš”
            rate_limit_manager: é¢‘ç‡é™åˆ¶ç®¡ç†å™¨

        Returns:
            dict: ä¸‹è½½ç»“æœ
        """
        result = {
            "symbol": symbol,
            "success": False,
            "records": 0,
            "error": None,
        }

        try:
            # è·å–æ•°æ®
            data = self._fetch_symbol_data(
                symbol=symbol,
                start_ts=start_ts,
                end_ts=end_ts,
                interval=interval,
                rate_limit_manager=rate_limit_manager,
            )

            if data:
                if self.db is None:
                    raise MarketDataFetchError("Database is not initialized")

                # å­˜å‚¨åˆ°æ•°æ®åº“
                self.db.store_data(data, interval)

                result.update(
                    {
                        "success": True,
                        "records": len(data),
                        "time_range": f"{data[0].open_time} - {data[-1].open_time}",
                    }
                )

                # ä½¿ç”¨è‡ªå®šä¹‰loggerè¿›è¡Œè¡Œå†…æ›´æ–°
                if logger.output_mode == OutputMode.COMPACT:
                    logger.print_inline(f"âœ… {symbol}: {len(data)} æ¡è®°å½•")
                else:
                    logger.debug(f"âœ… {symbol}: {len(data)} æ¡è®°å½•")

                # å¦‚æœæœ‰å¯ç”¨çš„è¿›åº¦æ¡ï¼Œæ›´æ–°è¿›åº¦
                try:
                    logger.update_symbol_progress(symbol, "å®Œæˆ")
                except Exception:
                    pass  # è¿›åº¦æ¡å¯èƒ½æœªå¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯
            else:
                result["error"] = "æ— æ•°æ®"
                if logger.output_mode == OutputMode.COMPACT:
                    logger.print_inline(f"âš ï¸ {symbol}: æ— æ•°æ®")
                else:
                    logger.debug(f"âš ï¸ {symbol}: æ— æ•°æ®")

        except InvalidSymbolError as e:
            result["error"] = f"æ— æ•ˆäº¤æ˜“å¯¹: {e}"
            logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆäº¤æ˜“å¯¹ {symbol}")

        except Exception as e:
            result["error"] = str(e)
            logger.error(f"âŒ {symbol} å¤±è´¥: {e}")

        return result
